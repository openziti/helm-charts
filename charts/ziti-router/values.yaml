
ha:
  # -- must be enabled if multiple controllers
  enabled: false

ctrl:
  # -- required control plane endpoint, e.g., ctrl.ziti.example.com:443
  endpoint: ""

# -- Explicit proxy setting in the router configuration. Router can be deployed in a site 
# where all egress traffic is forwarded through an explicit proxy.
# The enrollment will also be forwarded through the proxy.
proxy: {}
#  type: http
#  address: 192.168.0.142
#  port: 3128

# -- assign key=value in pod environment
env: {}
  # SOME_ENV: "true"

identity:
  altServerCerts: []
    #  # -- request an alternative server certificate from a cert-manager
    #  #    issuer; this mode does not require a matching additionalVolumes member
    #- mode: certManager
    #  # -- name of the secret in which to store the certificate and key
    #  secretName: ziti-router-alt-server-cert-1
    #  # -- name of the additional listener which advertised host will be the DNS SAN in the requested certificate
    #  additionalListenerName:
    #  # -- issuer ref to use when requesting the alternative server certificate
    #  issuerRef:
    #    group: cert-manager.io
    #    kind: ClusterIssuer
    #    name: cloudflare-dns01-issuer
    #  # -- where to mount the tls secret on the pod
    #  mountPath: /etc/ziti/alt-server-cert

    #  # -- use an alternative certificate and key from a tls secret declared in
    #  #    additionalVolumes using the mountPath specified there
    #- mode: secret
    #  # -- name of existing secret mounted by additionalVolumes
    #  secretName: ziti-router-alt-server-cert

    #  # -- manually configure a path where you will separately mount the
    #  #    alternative server certificate and key; localFile certs are skipped
    #  #    by the deployment template because they lack a secretName, so these
    #  #    paths must match a member of additionalVolumes
    #- mode: localFile
    #  # -- 'localFile': path to the alternative server certificate
    #  serverCert:
    #  # -- 'localFile': path to the alternative server key
    #  serverKey:

# listen for router links
linkListeners:
  transport:
    # -- enable the transport listener in the router config; set false for a
    # private router that only connects to other routers and does not accept
    # incoming links
    enabled: true
    # -- cluster service target port on the container; default is to share a listener with the edge, providing ziti-link ALPN
    containerPort: '{{ .Values.edge.containerPort }}'
    # -- DNS name that other routers will use to form mesh transport links to this listener
    advertisedHost: '{{ .Values.edge.advertisedHost }}'
    # -- cluster service, node port, load balancer, and ingress port for this listener; default is edge.advertisedPort
    advertisedPort: '{{ .Values.edge.advertisedPort }}'
    # -- link listener options
    options: {}
    service:
      # -- create a cluster service for the router transport link listener;
      # unnecessary if advertisedHost is shared with edge listener (the default)
      enabled: true  # default enabled for backward compatibility
      # -- expose the service as a ClusterIP, NodePort, or LoadBalancer
      type: ClusterIP
      # -- service labels
      labels: {}
      # -- service annotations
      annotations: {}
    ingress:
      # -- create an ingress for the cluster service
      enabled: false
      # -- ingress class name
      ingressClassName: ""
      # -- ingress labels
      labels: {}
      # -- ingress annotations, e.g., to configure ingress-nginx
      annotations: {}

# listen for edge clients
edge:
  # -- enable the edge listener in the router config; usually true because
  # tunnel bindings require the edge which must have at least on listener
  enabled: true
  # -- edge listener protocol: tls, wss; usually tls because additionalListeners
  # can be used to provide a wss listener
  protocol: tls
  # -- cluster service target port on the container
  containerPort: 3022
  # -- Domain name that edge clients will use to reach this router's edge
  # listener. Default is cluster-internal service DNS name:port.
  advertisedHost: ""
  # -- cluster service, node port, load balancer, and ingress port
  advertisedPort: 443
  # -- additional common xgress options
  options: {}
  service:
    # -- create a cluster service for the edge listener; usually true, but you
    # can disable this to effectively un-publish the edge listener
    enabled: true
    # -- expose the service as a ClusterIP, NodePort, or LoadBalancer; default
    # is ClusterIP, but you could use NodePort or LoadBalancer instead of an
    # ingress controller
    type: ClusterIP
    # -- service labels
    labels: {}
    # -- service annotations
    annotations: {}
  ingress:
    # -- create an ingress for the cluster service; typically paired with a
    # ClusterIP service type when enabled
    enabled: false
    # -- ingress class name
    ingressClassName: ""
    # -- ingress labels
    labels: {}
    # -- ingress annotations, e.g., to configure ingress-nginx for passthrough TLS
    annotations: {}
  # -- additional edge listeners have the same shape as the default edge
  # listener, except they're enabled if defined, and you must specify a unique
  # name for each additional edge listener. The name distinguishes their
  # respective cluster services. This is useful for BrowZer clients that require
  # a trusted certificate for the edge WebSocket and advertisedHost must resolve
  # to a public IP that presents a trusted certificate, e.g., an Ingress with
  # TLS termination.
  additionalListeners: []
    #- name: router11-edge-wss
    #  protocol: wss
    #  # -- wss cluster service target port on the container must be distinct
    #  from the default edge listener so that both can have a default protocol
    #  handler, i.e., h2,http/1.1 is ambiguous if REST healthchecks share the
    #  containerPort with default edge listener
    #  containerPort: 3023
    #  # -- host where clients connect, e.g., ingress external IP or DNS name
    #  advertisedHost:
    #  # -- advertised port must be 443 if ingress is enabled
    #  advertisedPort: 443
    #  # -- Should this additional lister advertised Host be added to SAN of the
    #  identity? Depends on use-case. i.e. for a WSS server you won't do that
    #  and provide an additional cert for this endpoint.
    #  addHostToSan: false
    #  # -- additional edge listener options
    #  options: {}
    #  # -- additional edge listeners can have their own cluster services
    #  service:
    #    enabled: true
    #    type: ClusterIP
    #    labels: {}
    #    annotations: {}
    #  # -- additional edge listeners can have their own ingresses
    #  ingress:
    #    enabled: false
    #    # -- ingress must passthrough tls to to wss listener
    #    annotations: {}
    #    labels: {}
    #    ingressClassName: ""

tunnel:
  # -- run mode for the router's built-in tunnel component: host, tproxy, proxy, or none
  mode: none
  # -- Ziti nameserver listener where OS must be configured to send DNS queries (default: udp://127.0.0.1:53)
  resolver:
  # -- CIDR range for the internal service fqdn to dynamic intercept IP address resolution (default: 100.64.0.0/10)
  dnsSvcIpRange:
  # -- interface device name for setting up INPUT firewall rules if fw enabled.
  # It must be set but not needed in containers. Thus, it is set to lo by default
  lanIf: lo
  # -- the tproxy mode can be switched from iptables based interception to bpf interception by passing
  # the user space bpf program path. bpf kernel space program is expected to be loaded prior or during
  # router deployment, e.g. bpfman agent, hostpath, etc
  diverterPath:
  # -- list of Ziti services for which K8s services are to be created by this deployment, default is one cluster service port per Ziti service
  proxyServices: []
    #  # -- name of the Openziti service to forward data to
    #- zitiService: my_ziti_service
    #  # -- name suffix when not using the "default" proxy tunnel service, see above: "additionalK8sServices"
    #  k8sService: my_k8s_suffix
    #  # -- port the kubernetes service uses as listen port
    #  advertisedPort: 80
    #  # -- override the container port (default: advertisedPort)
    #  containerPort: 8443
  # -- if tunnel mode is "proxy", create the a cluster service named {{ release }}-proxy-default listening on each "advertisedPort" defined in "proxyServices"
  proxyDefaultK8sService:
    enabled: true
    type: ClusterIP
  # -- if tunnel mode is "proxy", create a separate cluster service for each Ziti service listed in "proxyServices" which k8sService == name
  proxyAdditionalK8sServices: []
    #- name: myservice
    #  type: ClusterIP

# websocket related config
websocket:
  # -- enable the websocket transport. Also requires an appropriate edge.additionalListeners entry.
  enabled: false
  # -- websocket write timeout
  writeTimeout: 10
  # -- websocket read timeout
  readTimeout: 5
  # -- websocket idle timeout
  idleTimeout: 5
  # -- websocket pong timeout
  pongTimeout: 60
  # -- websocket ping timeout
  pingInterval: 54
  # -- websocket handshake timeout
  handshakeTimeout: 10
  # -- websocket read buffer size
  readBufferSize: 4096
  # -- websocket write buffer size
  writeBufferSize: 4096
  # -- enable compression on websocket
  enableCompression: true

# -- Certificate signing request distinguished name and subject alternative names
csr:
  # -- country
  country: ""
  # -- state
  province: ""
  # -- city
  locality: ""
  # -- organization
  organization: ""
  # -- organizational unit
  organizationalUnit: ""
  sans:
    # -- if true, disable computing default SANs from the advertisedHost, etc.
    noDefaults: false
    # -- additional DNS SANs
    dns: []
    # -- additional IP SANs
    ip: []
    # -- additional email SANs
    email: []
    # -- additional URI SANs
    uri: []

# -- read-only mountpoint for executables (must be in image's executable search PATH)
execMountDir:     /usr/local/bin
# -- enrollment one time token from the controller's management API
enrollmentJwt:
# -- allow for using a secret to specify the enrollment token instead of unsing the enrollmentJwt field
# if enabled, setting the enrollment token on the enrollmentJwt field has no effect
enrollmentJwtFromSecret: false
# -- set the enrollment jwt from a secret
# The enrollment token secret must be of the following format:
# apiVersion: v1
# kind: Secret
# metadata:
#   name: myEnrollmentJwtSecret
# type: Opaque
# data:
#   enrollmentJwt:
enrollmentJwtSecretName: ""
# -- read-only mountpoint for router identity secret specified in deployment for use by router run container
identityMountDir: /etc/ziti/identity
# -- writeable mountpoint where read-only config file is projected to allow router
# to write ./endpoints statefile in same dir
configMountDir:   /etc/ziti/config
# -- filename of router config YAML
configFile:       ziti-router.yaml

image:
  # -- container image tag for deployment
  repository: docker.io/openziti/ziti-router
  # -- container image tag (default is Chart's appVersion)
  tag:
  # -- deployment image pull policy
  pullPolicy: Always
  # pullSecrets:
  # command: ["sh", "-c", "while true; do sleep 86400; done"]
  # -- deployment container command
  command: ['/entrypoint.bash']
  # The templates inside this list are evaluated by the tpl function in the
  # deployment template because Helm values can not be directly referenced
  # inside the Values.yaml file (because it's just a data structure, not a
  # template).
  # -- deployment container command args and opts
  args: [ 'run', '{{ .Values.configMountDir }}/{{ .Values.configFile }}']
  # -- additional arguments can be passed directly to the container to modify ziti runtime arguments
  additionalArgs: []

# deployment DNS policy
dnsPolicy: ClusterFirstWithHostNet

# -- it allows to override dns options when dnsPolicy is set to None.
dnsConfig: {}
#  nameservers: []
#  searches: []
#  options: []

# nameOverride: ""
# fullnameOverride: ""

# -- additional volumes to mount to ziti-router container
additionalVolumes: []
#  - name: additional-volume-1
#    volumeType: secret
#    mountPath: /path/to/mount/1
#    secretName: name-of-secret
#  - name: additional-volume-2
#    volumeType: configMap
#    defaultMode: 0644
#    mountPath: /path/to/configmap/mount
#    configMapName: your-configmap-name
#  - name: additional-volume-3
#    volumeType: csi
#    driverName: csi.bpfd.dev
#    attributes: volumeAttributes
#    mountPath: /path/to/mount/3
#  - name: additional-volume-4
#    volumeType: emptyDir
#    mountPath: /path/to/mount/4
#  - name: additional-volume-5
#    volumeType: hostPath
#    behavior: DirectoryOrCreate
#    mountPath: /path/to/mount/4
#

# -- annotations to apply to all pods deployed by this chart
podAnnotations: {}

# -- deployment template spec security context
podSecurityContext:
  # -- this is the GID of "ziggy" run-as user in the container that has access
  # to any files created by the router process in the emptyDir volume used to
  # persist the list of ctrl endpoints
  fsGroup: 2171

# -- Host networking requested for a pod if set, i.e. tproxy ports enabled in the host namespace.
# i.e. egress gateway
hostNetwork: False

# -- deployment container security context
securityContext:
  # capabilities:
  #   add:
  #     - NET_ADMIN

# -- deployment container resources
resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# -- deployment template spec node selector
nodeSelector: {}
#  kubernetes.io/role: master

# -- deployment template spec tolerations
tolerations: []
  # - key: node-role.kubernetes.io/master
  #   operator: Exists
  #   effect: NoSchedule

# -- deployment template spec affinity
affinity: {}

## Enable persistence using Persistent Volume Claims
## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
##
persistence:
  # -- required: place a storage claim for the ctrl endpoints state file
  enabled: true
  # -- annotations for the PVC
  annotations: {}
  # -- A manually managed Persistent Volume and Claim
  # Requires persistence.enabled: true
  # If defined, PVC must be created manually before volume will be bound
  existingClaim: ""
  ## minio data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # -- Storage class of PV to bind. By default it looks for the default storage class.
  # If the PV uses a different storage class, specify that here.
  storageClass: ""
  # -- PVC volume name
  volumeName:
  # -- PVC access mode: ReadWriteOnce (concurrent mounts not allowed), ReadWriteMany (concurrent allowed)
  accessMode: ReadWriteOnce
  # -- 50Mi is plenty for this state file 
  size: 50Mi
fabric:
  metrics:
    # -- configure fabric metrics in the router config
    enabled: false

forwarder:
  latencyProbeInterval: 10
  xgressDialQueueLength: 1000
  xgressDialWorkerCount: 128
  linkDialQueueLength: 1000
  linkDialWorkerCount: 32
  rateLimitedQueueLength: 5000
  rateLimitedWorkerCount: 64
