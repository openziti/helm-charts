
ctrl:
  # -- required control plane endpoint
  endpoint: # ctrl.example.com:6262

# Explicit proxy setting in the router configuration. Router can be deployed in a site where all egress traffic is forwarded through an explicit proxy.
# The enrollment will also be forwarded through the proxy
proxy: {}
#  type: http
#  address: 192.168.0.142
#  port: 3128

# -- common advertise-host for transport and edge listeners can also be
# specified separately via `edge.advertisedHost` and
# `linkListeners.transport.advertisedHost`
advertisedHost:

# Certificate signing request basic data
csr:
  # country: US
  # province: NC
  # locality: Charlotte
  # organization: NetFoundry
  # organizationalUnit: Ziti
  sans:
  # # you could specify additional SANS here - configurations from advertise hosts and
  # # service's will be collected automatically
  # -- additional DNS SANs
    dns: []
  #     - my.additional.host
  # -- additional IP SANs
    ip: []
  #     - "192.168.10.0"


# listen for router links
linkListeners:
  transport:  # https://docs.openziti.io/docs/reference/configuration/router/#transport
    # -- cluster service target port on the container
    containerPort: 10080
    # -- DNS name that other routers will use to form mesh transport links with this router. Default is cluster-internal service DNS name:port.
    advertisedHost: #router11-transport.router-namespace.svc:443
    # -- cluster service, node port, load balancer, and ingress port
    advertisedPort: 443
    service:
      # -- create a cluster service for the router transport link listener
      enabled: true
      # -- expose the service as a ClusterIP, NodePort, or LoadBalancer
      type: ClusterIP
      # -- service labels
      labels:
      # -- service annotations
      annotations:
    ingress:
      # -- create an ingress for the cluster service
      enabled: false
      # -- ingress annotations, e.g., to configure ingress-nginx
      annotations:

# listen for edge clients
edge:
  # -- enable the edge listener in the router config
  enabled: true
  # -- cluster service target port on the container
  containerPort: 3022
  # -- DNS name that edge clients will use to reach this router's edge listener
  advertisedHost: #router11-edge.ziti.example.com
  # -- cluster service, node port, load balancer, and ingress port
  advertisedPort: 443
  service:
    # -- create a cluster service for the edge listener
    enabled: true
    # -- expose the service as a ClusterIP, NodePort, or LoadBalancer
    type: ClusterIP
    # -- service labels
    labels:
    # -- service annotations
    annotations:
  ingress:
    # -- create an ingress for the cluster service
    enabled: false
    # -- ingress annotations, e.g., to configure ingress-nginx
    annotations:

tunnel:
  # -- run mode for the router's built-in tunnel component: host, tproxy, proxy, or none
  mode: host
  # -- built-in nameserver configuration, e.g. udp://127.1.2.3:53
  resolver: none
  # lanIf: eth0   # interface device name for tproxy?
  # diverterPath: /path/to/userspace/bpf-program
  # -- list of definitions to configure proxy services and according kubernetes services for these if mode "proxy"
  proxyServices: []
    #  # -- name of the Openziti service to forward data to
    #- zitiService: my_service.svc
    #  # -- port the router opens to listen for data
    #  containerPort: 8443
    #  # -- port the kubernetes service uses as listen port
    #  advertisedPort: 80
    #  # -- name suffix when not using the "default" proxy tunnel service, see above: "additionalK8sServices"
    #  k8sservice: myservice
  # -- default Kubernetes service object listening to proxy ports defined in "proxyServices", only if tunnel mode is "proxy"
  proxyDefaultK8sService:
    enabled: true
    type: ClusterIP
  # -- additional Kubernetes services created additionally to the "default" proxy listener service, only if mode is "proxy"
  proxyAdditionalK8sServices: []
    #- name: myservice
    #  type: ClusterIP

# -- read-only mountpoint for executables (must be in image's executable search PATH)
execMountDir:     /usr/local/bin
# -- exec by Helm post-install hook
initScriptFile:   ziti-router-init.bash
# -- exec by Helm post-delete hook
deleteIdentityScriptFile: delete-identity.bash
# -- projected subpath where the enrollment token will be mounted
enrollJwtFile: enrollment.jwt
# -- enrollment one time token from the controller's management API
enrollmentJwt:
# -- read-only mountpoint for router identity secret specified in deployment for use by router run container
identityMountDir: /etc/ziti/identity
# -- writeable mountpoint where read-only config file is projected to allow router
# to write ./endpoints statefile in same dir
configMountDir:   /etc/ziti/config
# -- filename of router config YAML
configFile:       ziti-router.yaml

image:
  # -- container image tag for deployment
  repository: docker.io/openziti/ziti-router
  # -- deployment image pull policy
  pullPolicy: Always
  # pullSecrets:
  # command: ["sh", "-c", "while true; do sleep 86400; done"]
  # -- deployment container command
  command: ["ziti", "router", "run"]
  # The templates inside this list are evalated by the tpl function in the
  # deployment template because Helm values can not be directly referenced
  # inside the Values.yaml file (because it's just a data structure, not a
  # template).
  # -- deployment container command args and opts
  args: [ "{{ .Values.configMountDir }}/{{ .Values.configFile }}"]

# deployment DNS policy
dnsPolicy: ClusterFirstWithHostNet

# if dnsPolicy is None:
dnsConfig: {}
#  nameservers: []
#  searches: []
#  options: []

# nameOverride: ""
# fullnameOverride: ""

# -- additional volumes to mount to ziti-router container
additionalVolumes: []
#  - name: additional-volume-1
#    volumeType: emptyDir
#    mountPath: /path/to/mount/1
#    secretName: name-of-secret
#  - name: additional-volume-2
#    volumeType: configMap
#    defaultMode: 0644
#    mountPath: /path/to/configmap/mount
#    configMapName: your-configmap-name
#  - name: additional-volume-3
#    volumeType: csi
#    driverName: csi.bpfd.dev
#    attributes: volumeAttributes
#    mountPath: /path/to/mount/3
#  - name: additional-volume-4
#    volumeType: emptyDir 
#    mountPath: /path/to/mount/4
#  - name: additional-volume-5
#    volumeType: hostPath 
#    behavior: DirectoryOrCreate
#    mountPath: /path/to/mount/4
#

# -- annotations to apply to all pods deployed by this chart
podAnnotations: {}

# -- deployment template spec security context
podSecurityContext:
  # -- this is the GID of "ziggy" run-as user in the container that has access
  # to any files created by the router process in the emptyDir volume used to
  # persist the endpoints state file
  fsGroup: 2171

# Host networking requested for a pod, interception at the node level required
# i.e. egress gateway
hostNetwork: False

# -- deployment container security context
securityContext:
  # capabilities:
  #   add:
  #     - NET_ADMIN

# -- deployment container resources
resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# -- deployment template spec node selector
nodeSelector: {}
#  kubernetes.io/role: master

# -- deployment template spec tolerations
tolerations: []
  # - key: node-role.kubernetes.io/master
  #   operator: Exists
  #   effect: NoSchedule

# -- deployment template spec affinity
affinity: {}

## Enable persistence using Persistent Volume Claims
## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
##
persistence:
  # -- required: place a storage claim for the ctrl endpoints state file
  enabled: true
  # -- annotations for the PVC
  annotations: {}
  # -- A manually managed Persistent Volume and Claim
  # Requires persistence.enabled: true
  # If defined, PVC must be created manually before volume will be bound
  existingClaim: ""
  ## minio data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # -- Storage class of PV to bind. By default it looks for the default storage class.
  # If the PV uses a different storage class, specify that here.
  storageClass: ""
  # -- PVC volume name
  VolumeName:
  # -- PVC access mode: ReadWriteOnce (concurrent mounts not allowed), ReadWriteMany (concurrent allowed)
  accessMode: ReadWriteOnce
  # -- 50Mi is plenty for this state file 
  size: 50Mi
fabric:
  metrics:
    # -- configure fabric metrics in the router config
    enabled: false
