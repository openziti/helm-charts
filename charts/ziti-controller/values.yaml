# Default values for ziti-controller.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- permanent SPIFFE ID to use for this controller's trust domain (default: random, fixed for the life of the chart release)
trustDomain: ""

clientApi:
  # -- cluster service target port on the container
  containerPort: 1280
  # -- global DNS name by which routers can resolve a reachable IP for this service
  advertisedHost: ""
  # -- cluster service, node port, load balancer, and ingress port
  advertisedPort: 443
  # -- besides advertisedHost, add these DNS SANs to the web identity and any ingresses
  dnsNames: []
  # -- besides advertisedHost and dnsNames, add these DNS SANs to any ingresses but not the web identity
  altDnsNames: []
  service:
    # -- create a cluster service for the deployment
    enabled: true
    # -- expose the service as a ClusterIP, NodePort, or LoadBalancer
    type: LoadBalancer  # this is the only service that really needs to be exposed
  ingress:
    # -- create a TLS-passthrough ingress for the client API's ClusterIP service
    enabled: false
    # -- ingress class name, e.g., "nginx"
    ingressClassName: ""
    # -- ingress labels
    labels: {}
    # -- ingress annotations, e.g., to configure ingress-nginx
    annotations: {}
    # -- deprecated: tls passthrough is required
    tls: {}
  traefikTcpRoute:
    # -- enable Traefik IngressRouteTCP
    enabled: false
    # -- IngressRouteTCP entrypoints
    entryPoints:
      - websecure
    # -- IngressRouteTCP labels
    labels: {}

# -- by default, there's no need for a separate cluster service, ingress, or
# load balancer for the management API because it shares a TLS listener with the
# client API, and is reachable at the same address and presents the same web
# identity cert; you may configure a separate service, ingress, load balancer,
# etc.  for the management API by setting managementApi.service.enabled=true
managementApi:
  # -- cluster service target port on the container
  containerPort: 1281
  # -- global DNS name by which routers can resolve a reachable IP for this service
  advertisedHost: "{{ .Values.clientApi.advertisedHost }}"
  # -- cluster service, node port, load balancer, and ingress port
  advertisedPort: "{{ .Values.clientApi.advertisedPort }}"
  # -- besides advertisedHost, add these DNS SANs to the web identity and any mgmt api ingresses
  dnsNames: []
  # -- besides advertisedHost and dnsNames, add these DNS SANs to any mgmt api ingresses, but not the web identity
  altDnsNames: []
  service:
    # -- create a cluster service for the deployment
    enabled: false   # enabled: true means provide this API on a separate port, otherwise share server port with clientApi
    # -- expose the service as a ClusterIP, NodePort, or LoadBalancer
    type: ClusterIP  # this doesn't need to be exposed if you exclusively manage with ZAC also running in the same cluster
  ingress:
    # -- create a TLS-passthrough ingress for the client API's ClusterIP service
    enabled: false
    # -- ingress class name, e.g., "nginx"
    ingressClassName: ""
    # -- ingress labels
    labels: {}
    # -- ingress annotations, e.g., to configure ingress-nginx
    annotations: {}
    # -- deprecated: tls passthrough is required
    tls: {}
  traefikTcpRoute:
    # -- enable Traefik IngressRouteTCP
    enabled: false
    # -- IngressRouteTCP entrypoints
    entryPoints:
      - websecure
    # -- IngressRouteTCP labels
    labels: {}

# by default, there's no need for a separate cluster service, ingress, or
# load balancer for the ctrl plane because it shares a TLS listener with the
# client API, advertising the same host and port but presenting the separate
# ctrl plane identity to TLS clients via ALPN; you may override these templates
# with literal host and port values and configure a separate service, ingress,
# load balancer, etc. for the ctrl plane
ctrlPlane:
  # -- cluster service target port on the container
  containerPort: "{{ .Values.clientApi.containerPort }}"
  # -- global DNS name by which routers can resolve a reachable IP for this
  # service: default is cluster service DNS name which assumes all routers are
  # inside the same cluster
  advertisedHost: "{{ .Values.clientApi.advertisedHost }}"
  # -- cluster service, node port, load balancer, and ingress port
  advertisedPort: "{{ .Values.clientApi.advertisedPort }}"
  # -- besides advertisedHost, add these DNS SANs to the ctrl plane identity and any ctrl plane ingresses
  dnsNames: []
  service:
    # -- create a separate cluster service for the ctrl plane; enabling this
    # requires you to also set the host and port for a separate ctrl plane TLS
    # listener
    enabled: false
    # -- expose the service as a ClusterIP, NodePort, or LoadBalancer
    type: ClusterIP
  ingress:
    # -- create an ingress for the cluster service
    enabled: false
    # -- ingress class name, e.g., "nginx"
    ingressClassName: ""
    # -- ingress labels
    labels: {}
    # -- ingress annotations, e.g., to configure ingress-nginx
    annotations: {}
    # -- deprecated: tls passthrough is required
    tls: {}
  # -- obtain the ctrl plane identity from an existing issuer instead of generating a new PKI
  alternativeIssuer: {}
  traefikTcpRoute:
    # -- enable Traefik IngressRouteTCP
    enabled: false
    # -- IngressRouteTCP entrypoints
    entryPoints:
      - websecure
    # -- IngressRouteTCP labels
    labels: {}

# -- override the address printed in Helm release notes if you configured an alternative DNS SAN for the console
consoleAltIngress: {}
  # host: ""
  # port: 443

# -- set name to value in containers' environment
env: {}
# SOME_ENV: "true"

# -- set secrets as environment variables in the container
envSecrets: {}
#  - name: SOME_SECRET_ENV
#    valueFrom:
#      secretKeyRef:
#        name: some-secret
#        key: some_secret_key


# -- allow for using a custom admin secret, which has to be created beforehand
# if enabled, the admin secret will not be generated by this Helm chart
useCustomAdminSecret: false

# -- set the admin user and password from a custom secret
# The custom admin secret must be of the following format:
# apiVersion: v1
# kind: Secret
# metadata:
#   name: myCustomAdminSecret
# type: Opaque
# data:
#   admin-user:
#   admin-password:
customAdminSecretName: ""

prometheus:
  # -- cluster service target port on the container
  containerPort: 9090
  # -- cluster service, node port, load balancer, and ingress port
  advertisedPort: 443
  # -- DNS name to advertise in place of the default internal cluster name built from the Helm release name
  advertisedHost: ""
  service:
    # -- create a cluster service for the deployment
    enabled: false
    # -- expose the service as a ClusterIP, NodePort, or LoadBalancer
    type: ClusterIP
    # -- extra labels for matching only this service, ie. serviceMonitor
    labels:
      app: "prometheus"
    annotations: {}

  # ServiceMonitor configuration
  serviceMonitor:
    # -- If enabled, and prometheus service is enabled, ServiceMonitor resources for Prometheus Operator are created
    enabled: true
    # -- Alternative namespace for ServiceMonitor resources
    namespace: null
    # -- Namespace selector for ServiceMonitor resources
    namespaceSelector: {}
    # -- ServiceMonitor annotations
    annotations: {}
    # -- Additional ServiceMonitor labels
    labels: {}
    # -- ServiceMonitor scrape interval
    interval: null
    # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)
    scrapeTimeout: null
    # -- ServiceMonitor relabel configs to apply to samples before scraping
    # (defines `relabel_configs`;  [reference](https://prometheus.io/docs/prometheus/latest/configuration/configuration))
    relabelings: []
    # -- ServiceMonitor relabel configs to apply to samples as the last step before ingestion ([reference](https://prometheus.io/docs/prometheus/latest/configuration/configuration))
    metricRelabelings: []
    # -- ServiceMonitor will add labels from the service to the Prometheus metric ([reference](https://prometheus.io/docs/prometheus/latest/configuration/configuration))
    targetLabels: []
    # -- ServiceMonitor will use http by default, but you can pick https as well
    scheme: https
    # -- ServiceMonitor will use these tlsConfig settings to make the health check requests
    tlsConfig:
      # -- set TLS skip verify, because the SAN will not match with the pod IP
      insecureSkipVerify: true

ca:
  # Note: The renewBefore and duration fields must be specified using a Go
  # time.Duration string format, which does not allow the d (days) suffix.
  # You must specify these values using s, m, and h suffixes instead.
  # duration: 2160h # 90d
  # renewBefore: 360h # 15d
  # -- Go time.Duration string format
  duration: 87840h # 3660d / 10y
  # -- Go time.Duration string format
  renewBefore: 720h # 30d
  # -- Set a custom cluster domain if other than cluster.local
  clusterDomain: "cluster.local"

cert:
  # Note: The renewBefore and duration fields must be specified using a Go
  # time.Duration string format, which does not allow the d (days) suffix.
  # You must specify these values using s, m, and h suffixes instead.
  # TODO lower this value!
  # duration: 2160h   # 90d
  # renewBefore: 360h # 15d
  # -- server certificate duration as Go time.Duration string format
  duration: 87840h    # 3660d / 10 y
  # -- rewnew server certificates before expiry as Go time.Duration string format
  renewBefore: 720h   # 30d

# you can enable these if you want to use roots of trust that are separate from
# the main identity used by the ctrl plane
edgeSignerPki:
  # -- generate a separate PKI root of trust for the edge signer CA
  enabled: true
  # -- obtain the edge signer intermediate CA from an existing issuer instead of generating a new PKI
  alternativeIssuer: {}
  # -- type of alternative issuer: Issuer, ClusterIssuer
  # kind:
  # -- metadata name of the alternative issuer
  # name:
  admin_client_cert:
    # -- create a client certificate for the admin user
    enabled: false
    # -- admin client certificate duration as Go time.Duration
    duration: 8760h
    # -- renew admin client certificate before expiry as Go time.Duration
    renewBefore: 720h

webBindingPki:
  # -- generate a separate PKI root of trust for web bindings, i.e., client,
  # management, and prometheus APIs
  enabled: true
  altServerCerts: []
    #  # -- request an alternative server certificate from a cert-manager issuer
    #  mode: certManager
    #  # -- name of the tls secret for cert-manager to create and manage the server
    #  #    certificate and private key
    #  # -- request a certificate for these alternative names distinct from advertisedHost and dnsNames of the clientApi, managementApi, and ctrlPlane
    #  altDnsNames: []
    #  secretName: ziti-controller-alt-server-cert
    #  # -- issuer ref to use when requesting the alternative server certificate
    #  issuerRef:
    #    group: cert-manager.io
    #    kind: ClusterIssuer
    #    name: cloudflare-dns01-issuer
    #  # -- where to mount the tls secret on the pod - must not collide with another mountpoint
    #  mountPath: /etc/ziti/alt-server-cert

    #  # -- use an alternative certificate and key from a tls secret declared in additionalVolumes
    #- mode: secret
    #  # -- name of the tls secret from additionalVolumes with the alternative
    #  #    server certificate and private key
    #  secretName: ziti-controller-alt-server-cert

    #  # -- use an explicit file path to the alternative server certificate and key
    #- mode: localFile
    #  # -- 'localFile': path to the alternative server certificate
    #  serverCert:
    #  # -- 'localFile': path to the alternative server key
    #  serverKey:
  # -- obtain the web identity from an existing issuer instead of generating a new PKI
  alternativeIssuer: {}
  # -- type of alternative issuer: Issuer, ClusterIssuer
  # kind:
  # -- metadata name of the alternative issuer
  # name:

spireAgent:
  # -- if you are running a container with the spire-agent binary installed
  # then this will allow you to add the hostpath necessary for connecting to
  # the spire socket
  enabled: false
  # -- file path of the spire socket mount
  spireSocketMnt: /run/spire/sockets

image:
  # -- homeDir for admin login shell must align with container image's ~/.bashrc for ziti CLI auto-complete to work
  homeDir: /home/ziggy
  # -- container image repository for app deployment
  repository: docker.io/openziti/ziti-controller
  # -- override the container image tag specified in the chart
  tag: ""
  # -- deployment image pull policy
  pullPolicy: IfNotPresent
  # -- container entrypoint command
  command: ["ziti", "controller", "run"]
  # -- args for the entrypoint command
  args: ["{{ include \"configMountDir\" . }}/ziti-controller.yaml"]
  # -- additional arguments can be passed directly to the container to modify ziti runtime arguments
  additionalArgs: []

# -- name of the BoltDB file
dbFile:         ctrl.db

ctrlPlaneCasBundle:
  # -- namespaces where trust-manager will create the Bundle resource containing
  # Ziti's trusted CA certs (default: empty means all namespaces)
  namespaceSelector: {}
    # matchLabels:
    #   openziti.io/namespace: "enabled"
    # matchLabels:
    #   kubernetes.io/metadata.name: ziti


# nameOverride: ""
# fullnameOverride: ""

# -- additional volumes to mount to ziti-controller container
additionalVolumes: []
#  - name: additional-volume-1
#    volumeType: secret
#    mountPath: /path/to/mount/1
#    secretName: name-of-secret
#  - name: additional-volume-2
#    volumeType: configMap
#    mountPath: /path/to/configmap/mount
#    configMapName: your-configmap-name
#  - name: additional-volume-3
#    volumeType: emptyDir
#    mountPath: /path/to/mount/2

# -- annotations to apply to all pods deployed by this chart
podAnnotations: {}

# -- deployment template spec security context
podSecurityContext:
  # -- the GID of the group that should own any files created by the container, especially the BoltDB file
  fsGroup: 2171

# -- deployment container security context
securityContext: {}
  # capabilities:
  #   add:
  #     - NET_ADMIN

# -- deployment container resources
resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# -- deployment template spec node selector
nodeSelector: {}
#  kubernetes.io/role: master

# -- deployment template spec tolerations
tolerations: []
  # - key: node-role.kubernetes.io/master
  #   operator: Exists
  #   effect: NoSchedule

# -- deployment template spec affinity
affinity: {}

highAvailability:
  # -- Ziti controller HA mode
  mode: standalone
  # -- Ziti controller HA swarm replicas
  replicas: 1

## Enable persistence using Persistent Volume Claims
## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
##
persistence:
  # -- required: place a storage claim for the BoltDB persistent volume
  enabled: true
  # -- annotations for the PVC
  annotations: {}

  # -- A manually managed Persistent Volume and Claim Requires
  # persistence.enabled=true. If defined, PVC must be created manually before
  # volume will be bound.
  existingClaim: ""

  ## minio data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # -- Storage class of PV to bind. By default it looks for the default storage class.
  # If the PV uses a different storage class, specify that here.
  storageClass: ""
  # -- PVC volume name
  VolumeName: ""
  # -- PVC access mode: ReadWriteOnce (concurrent mounts not allowed), ReadWriteMany (concurrent allowed)
  accessMode: ReadWriteOnce
  # -- 2GiB is enough for tens of thousands of entities, but feel free to make it larger
  size: 2Gi

## DEPRECATED - This will be going away in a future release
## Use additionalConfigs block for event configuration
fabric:
  events:
    # -- enable fabric event logger and file handler
    enabled: false
    network:
      # -- matching interval age and reporting interval ensures coherent metrics from fabric events
      intervalAgeThreshold: 5s
      # -- matching interval age and reporting interval ensures coherent metrics from fabric events
      metricsReportInterval: 5s
    mountDir: /var/run/ziti
    fileName: fabric-events.json
    subscriptions:
      - type: fabric.circuits
      - type: fabric.links
      - type: fabric.routers
      - type: fabric.terminators
      - type: metrics
        sourceFilter: .*
        metricFilter: .*
      - type: edge.sessions
      - type: edge.apiSessions
      - type: fabric.usage  # used by zrok for limits enforcement
        version: 3
      - type: services
      - type: edge.entityCounts
        interval: 5s

network:
  # -- routeTimeoutSeconds controls the number of seconds the controller will wait for a route attempt to succeed.
  routeTimeoutSeconds:  10

  # -- createCircuitRetries controls the number of retries that will be attempted to create a path (and terminate it)
  # for new circuits.
  createCircuitRetries: 2

  # -- pendingLinkTimeoutSeconds controls how long we'll wait before creating a new link between routers where
  # there isn't an established link, but a link request has been sent
  pendingLinkTimeoutSeconds: 10

  # -- Defines the period that the controller re-evaluates the performance of all of the circuits
  # running on the network.
  cycleSeconds: 15

  # -- Sets router minimum cost. Defaults to 10
  minRouterCost: 10

  # -- Sets how often a new control channel connection can take over for a router with an existing control channel connection
  # Defaults to 1 minute
  routerConnectChurnLimit: 1m

  # -- Sets the latency of link when it's first created. Will be overwritten as soon as latency from the link is actually
  # reported from the routers. Defaults to 65 seconds.
  initialLinkLatency: 65s

  smart:
    # -- Defines the fractional upper limit of underperforming circuits that are candidates to be re-routed. If
    # smart routing detects 100 circuits that are underperforming, and `smart.rerouteFraction` is set to `0.02`,
    # then the upper limit of circuits that will be re-routed in this `cycleSeconds` period will be limited to
    # 2 (2% of 100).
    rerouteFraction: 0.02

    # -- Defines the hard upper limit of underperforming circuits that are candidates to be re-routed. If smart
    # routing detects 100 circuits that are underperforming, and `smart.rerouteCap` is set to `1`, and
    # `smart.rerouteFraction` is set to `0.02`, then the upper limit of circuits that will be re-routed in this
    # `cycleSeconds` period will be limited to 1.
    rerouteCap: 4

# A note about sub-charts: these values may serve as a partial reference for
# configuring the charts on which this chart depends.

# -- install the cert-manager subchart
cert-manager-enabled: false
cert-manager:
  # -- always use the cert-manager namespace because trust-manager assumes it is there
  namespace: cert-manager
  # -- clean up secret when certificate is deleted
  enableCertificateOwnerRef: true
  # -- CRDs must be applied in advance of installing the parent chart
  installCRDs: false

  # -- install the trust-manager subchart
trust-manager-enabled: false
trust-manager:
  namespace: cert-manager
  app:
    trust:
      # -- trust-manager needs to be configured to trust the namespace in which
      # the controller is deployed so that it will create the Bundle resource
      # for the ctrl plane trust bundle
      namespace:
  crds:
    # -- CRDs must be applied in advance of installing the parent chart
    enabled: false

# -- install the ingress-nginx subchart
ingress-nginx-enabled: false
ingress-nginx:
  controller:
    extraArgs:
      # -- configure subchart ingress-nginx to enable the pass-through TLS feature
      enable-ssl-passthrough: "true"

# -- Append additional config blocks in specific top-level keys: edge,
# web, network, ctrl. If events are defined here, they replace the default
# events section entirely.
additionalConfigs:
  ctrl: {}
  network: {}
  healthChecks: {}
  web: {}
  events: {}
  #    ampqLogger:
  #      subscriptions:
  #        - type: fabric.usage
  #          version: 3
  #      handler:
  #        type: amqp
  #        format: json
  #        url: "amqps://USER:PASSWORD@HOST:5671"
  #        queue: events
  #        # buffer size is how many events can be shipped per interval, larger networks need a larger buffer
  #        bufferSize: 1000
  #    jsonLogger:
  #      subscriptions:
  #        - type: metrics
  #          sourceFilter: .*
  #          metricFilter: .*
  #      handler:
  #        type: file
  #        format: json
  #        path: /var/run/ziti/metrics.log
  #        maxsizemb: 1024
  #        maxbackups: 3
